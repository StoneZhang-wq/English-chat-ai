# 可选：若出现「暂无可用场景」，可设置项目根目录的绝对路径（含 app/、data/ 的上一级）
# VOICE_CHAT_PROJECT_ROOT=/path/to/voice-chat-ai

# Conditional API Usage:
# Depending on the value of MODEL_PROVIDER, the corresponding service will be used when run.
# You can mix and match; use local Ollama with OpenAI speech or use OpenAI model with local TTS, etc.

# Model Provider: openai or ollama or xai or anthropic
MODEL_PROVIDER=openai

# Character to use - Options: alien_scientist, anarchist, ant_anarchist, bigfoot, bipolar_ai, capo_mio, chatgpt, clumsyhero, 
# conandoyle, conspiracy, cyberpunk, detective, dog, dream_weaver, drill_sergeant, einstein, elon_musk, femme_fatale, fight_club, 
# fitness_trainer, ghost, granny, grok_xai, hal9000, haunted_teddybear, insult, joker, method_actor, morpheus, mouse, mumbler, 
# nebula_barista, nerd, newscaster_1920s, noir_detective, paradox, pirate, retired_wrestler, revenge_deer, samantha, shadow_whisperer, 
# shakespeare, split, telemarketer, terminator, valleygirl, vampire, vato_loco, vegetarian_vampire, wizard, zombie_therapist, see character folder for more
CHARACTER_NAME=bigfoot

# Text-to-Speech (TTS) Configuration:
# TTS Provider - Options: sparktts (local uses the custom character .wav) or openai (uses OpenAI TTS voice) or elevenlabs or kokoro (your own selfhosted tts)
TTS_PROVIDER=openai

# Voice Speed for all TTS providers - 0.7 to 1.2, default is 1.0
VOICE_SPEED=1.0

# OpenAI TTS Voice - Used when TTS_PROVIDER is set to openai above
# Voice options: alloy, echo, fable, onyx, nova, shimmer, ash, coral, sage
OPENAI_TTS_VOICE=onyx
# 英语对话卡片：角色 B（用户）人声，不设则与上面相同。豆包 TTS 时用 TTS_VOICE_TYPE_B
# OPENAI_TTS_VOICE_B=nova

# 豆包 TTS：中文模式专用音色（与 AI 用中文确认学习场景时使用，必须为中文音色，如 zh_female_* / zh_male_*）
# TTS_VOICE_TYPE_ZH=zh_female_cancan_mars_bigtts

# OpenAI TTS Model-  NEW it uses emotions see https://www.openai.fm/ 
# Model options: gpt-4o-mini-tts, tts-1, tts-1-hd
OPENAI_MODEL_TTS=gpt-4o-mini-tts

# OpenAI Enhanced Mode Transcription Model
# Model options: gpt-4o-transcribe, gpt-4o-mini-transcribe, whisper-1
OPENAI_TRANSCRIPTION_MODEL=gpt-4o-mini-transcribe
# OpenAI Realtime model for WebRTC implementation, when playing games don't use the mini as the long prompt will cause it to forget 
# gpt-4o-realtime-preview , gpt-4o-mini-realtime-preview
OPENAI_REALTIME_MODEL=gpt-4o-realtime-preview-2024-12-17

# ElevenLabs Configuration:  (replace with your actual API key)
ELEVENLABS_API_KEY=your_api_key_here
# Default voice ID - find voice id's in your elevenlabs account
ELEVENLABS_TTS_VOICE=your_voice_id_here
# ElevenLabs TTS Model eleven_multilingual_v2 , eleven_flash_v2_5 is faster but less quality
ELEVENLABS_TTS_MODEL=eleven_multilingual_v2

# Kokoro TTS Configuration:
# bm_fable, bm_daniel, bm_lewis, af_alloy, af_bella
# See the kokoro web url ( if you have it installed ) for more voices http://localhost:8880/web/
KOKORO_TTS_VOICE=af_bella

# AUDIO GENERATION LENGTH
# Maximum character length for audio generation - set to 2000+ for stories and games, 3000 for assassin story, 4000 for mars encounter interactive
# MAX_CHAR_LENGTH is used for openai, elevenlabs and kokoro, is also used for max tokens for chat response, if MAX_CHAR_LENGTH is 500, then 500 * 4 // 3 = 666 max tokens is sent to provider
MAX_CHAR_LENGTH=3000

# OpenAI Configuration:
# gpt-4, gpt-4o-mini- gpt-4o, gpt-5-mini-2025-08-07
OPENAI_MODEL=gpt-4o
# OpenAI API Key for models and speech (replace with your actual API key)
OPENAI_API_KEY=your_openai_api_key_here

# Ollama Models Configuration:
# Models to use - OPTIONAL: For screen analysis, if MODEL_PROVIDER is ollama, llava will be used by default.
# Model to use - llama3.1 or 3.2 works well for local usage. In the UI it will get the list of models from /api/tags and display them. Not all models are supported.
# Uncencored model Godmoded/llama3-lexi-uncensored:latest  
OLLAMA_MODEL=llama3.2

# XAI Configuration:
# grok-2-1212, grok-3-mini-beta, grok-3-mini-fast-beta, grok-3-fast-beta, grok-3-beta, grok-4-1-fast-non-reasoning
XAI_MODEL=grok-4-1-fast-non-reasoning
XAI_API_KEY=your_api_key_here

# Anthropic Configuration: -  claude-3-7-sonnet-20250219
ANTHROPIC_MODEL=claude-3-7-sonnet-20250219
ANTHROPIC_API_KEY=your_api_key_here

# Local Transcription settings - true or false
# Set to false to skip loading Faster Whisper on startup and use OpenAI transcription
FASTER_WHISPER_LOCAL=false

# Endpoints:
# Set these below and no need to change often
OPENAI_BASE_URL=https://api.openai.com/v1/chat/completions
OPENAI_TTS_URL=https://api.openai.com/v1/audio/speech
OLLAMA_BASE_URL=http://localhost:11434
# IF RUNNING IN DOCKER CHANGE OLLAMA BASE URL TO THE ONE BELOW
# OLLAMA_BASE_URL=http://host.docker.internal:11434
XAI_BASE_URL=https://api.x.ai/v1
# Kokoro API base URL - default is localhost, change if running on another machine or in docker
# KOKORO_BASE_URL=http://host.docker.internal:8880/v1
KOKORO_BASE_URL=http://localhost:8880/v1
# For remote Kokoro TTS basic auth username and password if needed
# KOKORO_USERNAME=admin
# KOKORO_PASSWORD=test123

# Debug settings - true or false
# Set to true to enable extensive debug output 
DEBUG=false  
# Set to true to see audio level readings during recording           
DEBUG_AUDIO_LEVELS=false 

SPARKTTS_MODEL_DIR=pretrained_models/Spark-TTS-0.5B
SPARKTTS_MAX_CHARS=1000
# NOTES:
# List of trigger phrases to have the model view your desktop (desktop, browser, images, etc.).
# It will describe what it sees, and you can ask questions about it:
# "what's on my screen", "take a screenshot", "show me my screen", "analyze my screen", 
# "what do you see on my screen", "screen capture", "screenshot"
# To stop the conversation, say "Quit" or "Exit". ( ctl+c always works also)

# Suppress ALSA warnings in WSL2
# ALSA_CARD=default
# AUDIODEV=hw:0,0

# -----------------------------------------------------------------------------
# 语块/句型数据库（原表 chunk_core、scene_label，用于生成英语卡片）
# 首次使用：默认用 data/*.json 语块库，无需数据库
# 验证流程：python test_chunk_flow.py
# -----------------------------------------------------------------------------
# CHUNK_DB_HOST=127.0.0.1
# CHUNK_DB_PORT=3306
# CHUNK_DB_USER=root
# CHUNK_DB_PASSWORD=
# CHUNK_DB_NAME=english_chunk

# -----------------------------------------------------------------------------
# 用户记忆持久化（部署到 Railway 等无持久盘时建议用 supabase）
# 见 docs/SUPABASE_SCHEMA.sql 建表，scripts/migrate_to_supabase.py 迁移现有数据
# -----------------------------------------------------------------------------
# MEMORY_BACKEND=file
# MEMORY_BACKEND=supabase
# SUPABASE_URL=https://你的项目.supabase.co
# SUPABASE_SERVICE_ROLE_KEY=你的_service_role_密钥